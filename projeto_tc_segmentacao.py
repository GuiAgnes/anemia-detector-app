# -*- coding: utf-8 -*-
"""Projeto TC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L8REcHqF3uxzGgFMg4VHWJifvm3pWHox

Geração de Máscaras: Conversão das anotações do VGG Image Annotator (JSON) em imagens de máscara utilizáveis.

Treinamento do Modelo U-Net: Construção, compilação e treinamento da rede neural para segmentar a conjuntiva.

Aplicação do Modelo (Inferência): Uso do modelo treinado para analisar e segmentar novas imagens automaticamente.
"""

# Célula 1: Montar o Google Drive e instalar bibliotecas
try:
    from google.colab import drive
    print("Montando o Google Drive...")
    drive.mount('/content/drive', force_remount=True)
    COLAB_ENVIRONMENT = True
    print("\nConfiguração do Google Drive concluída com sucesso.")
except ImportError:
    print("Não estamos no Google Colab. Usando caminhos locais.")
    COLAB_ENVIRONMENT = False

# Instala o tensorflow caso não esteja na versão mais recente
!pip install -q tensorflow

import os
import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, concatenate, BatchNormalization, Activation
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import MobileNetV2
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import cv2
import json

# --- Configuração Global de Caminhos ---
if COLAB_ENVIRONMENT:
    DRIVE_BASE_PATH = '/content/drive/MyDrive/projeto_tc_ovinos/'
else:
    DRIVE_BASE_PATH = './'

os.makedirs(DRIVE_BASE_PATH, exist_ok=True)
IMAGES_PATH = os.path.join(DRIVE_BASE_PATH, 'imagens/')
MASKS_PATH = os.path.join(DRIVE_BASE_PATH, 'mascaras/')
JSON_PATH = os.path.join(DRIVE_BASE_PATH, 'FiltroManual.json')
MODEL_CHECKPOINT_PATH = os.path.join(DRIVE_BASE_PATH, 'melhor_modelo_unet_metricas_completas.keras')
GRAPH_SAVE_PATH = os.path.join(DRIVE_BASE_PATH, 'grafico_performance_metricas_completas.png')
NEW_IMAGES_PATH = os.path.join(DRIVE_BASE_PATH, 'fotos_para_analisar/')
RESULTS_PATH = os.path.join(DRIVE_BASE_PATH, 'resultados_segmentacao/')
RESULTS_MASKS_PATH = os.path.join(RESULTS_PATH, 'mascaras_previstas/')
RESULTS_OVERLAYS_PATH = os.path.join(RESULTS_PATH, 'recortes_previstos/')

# --- Configuração Global do Modelo ---
IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = 256, 256, 3
BATCH_SIZE, EPOCHS = 8, 150

# --- Métricas e Funções de Perda Personalizadas (Definidas Globalmente) ---
def dice_coefficient(y_true, y_pred, smooth=1):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

def dice_loss(y_true, y_pred):
    return 1 - dice_coefficient(y_true, y_pred)

def focal_loss(y_true, y_pred, alpha=0.8, gamma=2):
    y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1 - K.epsilon())
    cross_entropy = -y_true * K.log(y_pred)
    loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy
    return K.mean(loss)

def focal_dice_loss(y_true, y_pred):
    return focal_loss(y_true, y_pred) + dice_loss(y_true, y_pred)

def recall(y_true, y_pred, smooth=1e-6):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    true_positives = K.sum(y_true_f * y_pred_f)
    possible_positives = K.sum(y_true_f)
    recall = (true_positives + smooth) / (possible_positives + smooth)
    return recall

def precision(y_true, y_pred, smooth=1e-6):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    true_positives = K.sum(y_true_f * y_pred_f)
    predicted_positives = K.sum(y_pred_f)
    precision = (true_positives + smooth) / (predicted_positives + smooth)
    return precision

def f1_score(y_true, y_pred, smooth=1e-6):
    p = precision(y_true, y_pred, smooth)
    r = recall(y_true, y_pred, smooth)
    return (2 * (p * r) + smooth) / (p + r + smooth)

# --- NOVA MÉTRICA: IoU Focado no Foreground ---
def iou_metric(y_true, y_pred, smooth=1e-6):
    """
    Calcula o Intersection over Union (IoU) ou Jaccard Index.
    Foca apenas no foreground (mucosa).
    """
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)

    intersection = K.sum(y_true_f * y_pred_f)
    union = K.sum(y_true_f) + K.sum(y_pred_f) - intersection

    iou = (intersection + smooth) / (union + smooth)
    return iou

# Dicionário de objetos personalizados para carregar o modelo
CUSTOM_OBJECTS = {
    'focal_dice_loss': focal_dice_loss,
    'dice_coefficient': dice_coefficient,
    'iou_metric': iou_metric, # ATUALIZADO
    'precision': precision,
    'recall': recall,
    'f1_score': f1_score
}

print("Funções personalizadas e configurações globais definidas.")

# Célula 2: Gerador de Máscaras
print("\n--- Iniciando Bloco 2: Geração de Máscaras ---")

os.makedirs(MASKS_PATH, exist_ok=True)
try:
    with open(JSON_PATH) as f:
        data = json.load(f)
    annotations = data['_via_img_metadata']
    print(f"Encontradas anotações para {len(annotations)} imagens no arquivo JSON.")

    for image_id, ann in annotations.items():
        filename = ann['filename']
        image_path = os.path.join(IMAGES_PATH, filename)
        if not os.path.exists(image_path):
            print(f"AVISO: Imagem {filename} não encontrada. Pulando.")
            continue

        image = cv2.imread(image_path)
        if image is None:
            print(f"AVISO: Imagem {filename} corrompida ou ilegível. Pulando.")
            continue

        height, width, _ = image.shape
        mask = np.zeros((height, width), dtype=np.uint8)

        for region in ann['regions']:
            shape_attributes = region['shape_attributes']
            if shape_attributes['name'] == 'polygon':
                points = np.array([list(zip(shape_attributes['all_points_x'], shape_attributes['all_points_y']))], dtype=np.int32)
                cv2.fillPoly(mask, points, 255)

        mask_filename = os.path.splitext(filename)[0] + '.png'
        mask_path = os.path.join(MASKS_PATH, mask_filename)
        cv2.imwrite(mask_path, mask)
        # print(f"  - Máscara para {filename} gerada com sucesso.") # Comentado para reduzir o log

    print("\nProcesso de geração de máscaras concluído!")

except FileNotFoundError:
    print(f"ERRO: Arquivo JSON não encontrado em: {JSON_PATH}")
except Exception as e:
    print(f"Ocorreu um erro na Célula 2: {e}.")

print("\n--- Iniciando Bloco 3: Treinamento da U-Net ---")

# --- Carregamento dos Dados ---
print("--- Carregando imagens e máscaras ---")
image_files = sorted(os.listdir(IMAGES_PATH))
mask_files = sorted(os.listdir(MASKS_PATH))
X, y = [], []
for img_file in image_files:
    mask_file = os.path.splitext(img_file)[0] + '.png'
    if mask_file in mask_files:
        img = cv2.imread(os.path.join(IMAGES_PATH, img_file))
        if img is None: continue
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (IMG_HEIGHT, IMG_WIDTH))
        X.append(img)
        mask = cv2.imread(os.path.join(MASKS_PATH, mask_file), cv2.IMREAD_GRAYSCALE)
        if mask is None:
            X.pop()
            continue
        mask = cv2.resize(mask, (IMG_HEIGHT, IMG_WIDTH))
        y.append(mask)

X = np.array(X, dtype=np.float32)
X = tf.keras.applications.mobilenet_v2.preprocess_input(X)
y = np.array(y, dtype=np.float32) / 255.0
y = np.expand_dims(y, axis=-1)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"Dados carregados, pré-processados e divididos: {len(X_train)} para treino, {len(X_val)} para validação.")

# --- Aumento de Dados (Data Augmentation) ---
data_gen_args = dict(rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.05, zoom_range=0.1, horizontal_flip=True, brightness_range=[0.8, 1.2], fill_mode='nearest')
seed = 1
image_datagen = ImageDataGenerator(**data_gen_args)
mask_datagen = ImageDataGenerator(**data_gen_args)
image_datagen.fit(X_train, augment=True, seed=seed)
mask_datagen.fit(y_train, augment=True, seed=seed)
image_generator = image_datagen.flow(X_train, batch_size=BATCH_SIZE, seed=seed)
mask_generator = mask_datagen.flow(y_train, batch_size=BATCH_SIZE, seed=seed)
def combined_generator(img_gen, msk_gen):
    while True:
        yield (next(img_gen), next(msk_gen))
train_generator = combined_generator(image_generator, mask_generator)
print("\n--- Geradores de dados com augmentation configurados ---")

# --- U-Net com Encoder Pré-treinado (MobileNetV2) ---
def build_unet_with_transfer_learning(input_shape):
    base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')
    # NÃO congelamos o base_model aqui, faremos isso fora da função
    # base_model.trainable = False
    skip_connection_names = ['block_1_expand_relu', 'block_3_expand_relu', 'block_6_expand_relu', 'block_13_expand_relu', 'out_relu']
    encoder_outputs = [base_model.get_layer(name).output for name in skip_connection_names]
    encoder = Model(inputs=base_model.input, outputs=encoder_outputs)
    decoder_input = encoder.outputs[-1]
    d1 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(decoder_input); d1 = concatenate([d1, encoder.outputs[3]]); c1 = conv_block(d1, 512)
    d2 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c1); d2 = concatenate([d2, encoder.outputs[2]]); c2 = conv_block(d2, 256)
    d3 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c2); d3 = concatenate([d3, encoder.outputs[1]]); c3 = conv_block(d3, 128)
    # --- LINHAS CORRIGIDAS/ADICIONADAS ABAIXO ---
    d4 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c3); d4 = concatenate([d4, encoder.outputs[0]]); c4 = conv_block(d4, 64)
    d5 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c4)
    # --- FIM DA CORREÇÃO ---
    c5 = conv_block(d5, 32)
    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c5)
    model = Model(inputs=base_model.input, outputs=outputs)
    # Retornamos o base_model também para poder congelá-lo
    return model, base_model

def conv_block(input_tensor, num_filters):
    x = Conv2D(num_filters, (3, 3), kernel_initializer='he_normal', padding='same')(input_tensor); x = BatchNormalization()(x); x = Activation('relu')(x)
    x = Conv2D(num_filters, (3, 3), kernel_initializer='he_normal', padding='same')(x); x = BatchNormalization()(x); x = Activation('relu')(x)
    return x

# --- Treinamento ---
print("\n--- Iniciando Treinamento com Métricas Completas ---")
# Agora pegamos o base_model retornado
model, base_model = build_unet_with_transfer_learning((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))

# --- ETAPA 1: Treinamento do Decodificador ---
print("\n--- ETAPA 1: Treinamento inicial (Decodificador) ---")
base_model.trainable = False # Congelamos o encoder
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
              loss=focal_dice_loss,
              metrics=[
                  dice_coefficient,
                  iou_metric, # ATUALIZADO (removemos o MeanIoU)
                  precision,
                  recall,
                  f1_score
              ])
model.summary()

callbacks = [
    ModelCheckpoint(MODEL_CHECKPOINT_PATH, monitor='val_dice_coefficient', verbose=1, save_best_only=True, mode='max'),
    EarlyStopping(patience=25, monitor='val_dice_coefficient', mode='max', restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_dice_coefficient', factor=0.2, patience=10, min_lr=1e-6, verbose=1, mode='max')
]

history = model.fit(
    train_generator,
    steps_per_epoch=len(X_train) // BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=(X_val, y_val),
    callbacks=callbacks
)

print("\n--- Treinamento inicial concluído! ---")

# --- ETAPA 2: AJUSTE FINO (FINE-TUNING) ---
print("\n--- ETAPA 2: Iniciando Ajuste Fino (Fine-Tuning) ---")

# Descongelamos o modelo base
base_model.trainable = True

# Congelamos todas as camadas, exceto as últimas
fine_tune_at = 100 # Começa a treinar a partir da camada 100 do MobileNetV2
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

# Recompilamos o modelo com uma taxa de aprendizado MUITO BAIXA
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), # Taxa 10x menor
              loss=focal_dice_loss,
              metrics=[
                  dice_coefficient,
                  iou_metric,
                  precision,
                  recall,
                  f1_score
              ])

print("Modelo recompilado para ajuste fino.")
model.summary()

# Definimos novas épocas para o ajuste fino
# O EarlyStopping garantirá que ele pare se não houver melhoria
FINE_TUNE_EPOCHS = 50
total_epochs = EPOCHS + FINE_TUNE_EPOCHS

# Continuamos o treinamento (callbacks são reutilizados)
history_fine_tune = model.fit(
    train_generator,
    steps_per_epoch=len(X_train) // BATCH_SIZE,
    epochs=total_epochs,
    initial_epoch=history.epoch[-1], # Importante: continua de onde parou
    validation_data=(X_val, y_val),
    callbacks=callbacks # Os mesmos callbacks (incluindo ModelCheckpoint)
)

print("\n--- Ajuste Fino concluído! ---")

# --- Visualização da Performance (Combinando históricos) ---
# Adiciona o histórico do fine-tuning ao histórico inicial
for key in history_fine_tune.history:
    history.history[key].extend(history_fine_tune.history[key])

plt.figure(figsize=(20, 10))
plt.subplot(2, 3, 1); plt.plot(history.history['loss'], label='Perda Treino'); plt.plot(history.history['val_loss'], label='Perda Validação'); plt.title('Perda (Loss)'); plt.legend()
plt.subplot(2, 3, 2); plt.plot(history.history['iou_metric'], label='IoU Treino'); plt.plot(history.history['val_iou_metric'], label='IoU Validação'); plt.title('IoU (Foreground)'); plt.legend() # ATUALIZADO
plt.subplot(2, 3, 3); plt.plot(history.history['dice_coefficient'], label='Dice Coeff Treino'); plt.plot(history.history['val_dice_coefficient'], label='Dice Coeff Validação'); plt.title('Dice Coefficient'); plt.legend()
plt.subplot(2, 3, 4); plt.plot(history.history['precision'], label='Precision Treino'); plt.plot(history.history['val_precision'], label='Precision Validação'); plt.title('Precision'); plt.legend()
plt.subplot(2, 3, 5); plt.plot(history.history['recall'], label='Recall Treino'); plt.plot(history.history['val_recall'], label='Recall Validação'); plt.title('Recall'); plt.legend()
plt.subplot(2, 3, 6); plt.plot(history.history['f1_score'], label='F1-Score Treino'); plt.plot(history.history['val_f1_score'], label='F1-Score Validação'); plt.title('F1-Score'); plt.legend()
plt.tight_layout()
plt.savefig(GRAPH_SAVE_PATH)
print(f"Gráfico de performance salvo em: {GRAPH_SAVE_PATH}")
plt.show()

# Célula 3: Treinamento da U-Net (ATUALIZADA com Métricas Completas)
print("\n--- Iniciando Bloco 3: Treinamento da U-Net ---")

# --- Carregamento dos Dados ---
print("--- Carregando imagens e máscaras ---")
image_files = sorted(os.listdir(IMAGES_PATH))
mask_files = sorted(os.listdir(MASKS_PATH))
X, y = [], []
for img_file in image_files:
    mask_file = os.path.splitext(img_file)[0] + '.png'
    if mask_file in mask_files:
        img = cv2.imread(os.path.join(IMAGES_PATH, img_file))
        if img is None: continue
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (IMG_HEIGHT, IMG_WIDTH))
        X.append(img)
        mask = cv2.imread(os.path.join(MASKS_PATH, mask_file), cv2.IMREAD_GRAYSCALE)
        if mask is None:
            X.pop()
            continue
        mask = cv2.resize(mask, (IMG_HEIGHT, IMG_WIDTH))
        y.append(mask)

X = np.array(X, dtype=np.float32)
X = tf.keras.applications.mobilenet_v2.preprocess_input(X)
y = np.array(y, dtype=np.float32) / 255.0
y = np.expand_dims(y, axis=-1)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"Dados carregados, pré-processados e divididos: {len(X_train)} para treino, {len(X_val)} para validação.")

# --- Aumento de Dados (Data Augmentation) ---
data_gen_args = dict(rotation_range=15, width_shift_range=0.1, height_shift_range=0.1, shear_range=0.05, zoom_range=0.1, horizontal_flip=True, brightness_range=[0.8, 1.2], fill_mode='nearest')
seed = 1
image_datagen = ImageDataGenerator(**data_gen_args)
mask_datagen = ImageDataGenerator(**data_gen_args)
image_datagen.fit(X_train, augment=True, seed=seed)
mask_datagen.fit(y_train, augment=True, seed=seed)
image_generator = image_datagen.flow(X_train, batch_size=BATCH_SIZE, seed=seed)
mask_generator = mask_datagen.flow(y_train, batch_size=BATCH_SIZE, seed=seed)
def combined_generator(img_gen, msk_gen):
    while True:
        yield (next(img_gen), next(msk_gen))
train_generator = combined_generator(image_generator, mask_generator)
print("\n--- Geradores de dados com augmentation configurados ---")

# --- U-Net com Encoder Pré-treinado (MobileNetV2) ---
def build_unet_with_transfer_learning(input_shape):
    base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')
    # NÃO congelamos o base_model aqui, faremos isso fora da função
    # base_model.trainable = False
    skip_connection_names = ['block_1_expand_relu', 'block_3_expand_relu', 'block_6_expand_relu', 'block_13_expand_relu', 'out_relu']
    encoder_outputs = [base_model.get_layer(name).output for name in skip_connection_names]
    encoder = Model(inputs=base_model.input, outputs=encoder_outputs)
    decoder_input = encoder.outputs[-1]
    d1 = Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(decoder_input); d1 = concatenate([d1, encoder.outputs[3]]); c1 = conv_block(d1, 512)
    d2 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(c1); d2 = concatenate([d2, encoder.outputs[2]]); c2 = conv_block(d2, 256)
    d3 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c2); d3 = concatenate([d3, encoder.outputs[1]]); c3 = conv_block(d3, 128)
    # --- LINHAS CORRIGIDAS/ADICIONADAS ABAIXO ---
    d4 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c3); d4 = concatenate([d4, encoder.outputs[0]]); c4 = conv_block(d4, 64)
    d5 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c4)
    # --- FIM DA CORREÇÃO ---
    c5 = conv_block(d5, 32)
    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c5)
    model = Model(inputs=base_model.input, outputs=outputs)
    # Retornamos o base_model também para poder congelá-lo
    return model, base_model

def conv_block(input_tensor, num_filters):
    x = Conv2D(num_filters, (3, 3), kernel_initializer='he_normal', padding='same')(input_tensor); x = BatchNormalization()(x); x = Activation('relu')(x)
    x = Conv2D(num_filters, (3, 3), kernel_initializer='he_normal', padding='same')(x); x = BatchNormalization()(x); x = Activation('relu')(x)
    return x

# --- Treinamento ---
print("\n--- Iniciando Treinamento com Métricas Completas ---")
# Agora pegamos o base_model retornado
model, base_model = build_unet_with_transfer_learning((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))

# --- ETAPA 1: Treinamento do Decodificador ---
print("\n--- ETAPA 1: Treinamento inicial (Decodificador) ---")
base_model.trainable = False # Congelamos o encoder
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
              loss=focal_dice_loss,
              metrics=[
                  dice_coefficient,
                  iou_metric, # ATUALIZADO (removemos o MeanIoU)
                  precision,
                  recall,
                  f1_score
              ])
model.summary()

callbacks = [
    ModelCheckpoint(MODEL_CHECKPOINT_PATH, monitor='val_dice_coefficient', verbose=1, save_best_only=True, mode='max'),
    EarlyStopping(patience=25, monitor='val_dice_coefficient', mode='max', restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_dice_coefficient', factor=0.2, patience=10, min_lr=1e-6, verbose=1, mode='max')
]

history = model.fit(
    train_generator,
    steps_per_epoch=len(X_train) // BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=(X_val, y_val),
    callbacks=callbacks
)

print("\n--- Treinamento inicial concluído! ---")

# --- ETAPA 2: AJUSTE FINO (FINE-TUNING) ---
print("\n--- ETAPA 2: Iniciando Ajuste Fino (Fine-Tuning) ---")

# Descongelamos o modelo base
base_model.trainable = True

# Congelamos todas as camadas, exceto as últimas
fine_tune_at = 100 # Começa a treinar a partir da camada 100 do MobileNetV2
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

# Recompilamos o modelo com uma taxa de aprendizado MUITO BAIXA
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), # Taxa 10x menor
              loss=focal_dice_loss,
              metrics=[
                  dice_coefficient,
                  iou_metric,
                  precision,
                  recall,
                  f1_score
              ])

print("Modelo recompilado para ajuste fino.")
model.summary()

# Definimos novas épocas para o ajuste fino
# O EarlyStopping garantirá que ele pare se não houver melhoria
FINE_TUNE_EPOCHS = 50
total_epochs = EPOCHS + FINE_TUNE_EPOCHS

# Continuamos o treinamento (callbacks são reutilizados)
history_fine_tune = model.fit(
    train_generator,
    steps_per_epoch=len(X_train) // BATCH_SIZE,
    epochs=total_epochs,
    initial_epoch=history.epoch[-1], # Importante: continua de onde parou
    validation_data=(X_val, y_val),
    callbacks=callbacks # Os mesmos callbacks (incluindo ModelCheckpoint)
)

print("\n--- Ajuste Fino concluído! ---")

# --- Visualização da Performance (Combinando históricos) ---
# Adiciona o histórico do fine-tuning ao histórico inicial
for key in history_fine_tune.history:
    history.history[key].extend(history_fine_tune.history[key])

plt.figure(figsize=(20, 10))
plt.subplot(2, 3, 1); plt.plot(history.history['loss'], label='Perda Treino'); plt.plot(history.history['val_loss'], label='Perda Validação'); plt.title('Perda (Loss)'); plt.legend()
plt.subplot(2, 3, 2); plt.plot(history.history['iou_metric'], label='IoU Treino'); plt.plot(history.history['val_iou_metric'], label='IoU Validação'); plt.title('IoU (Foreground)'); plt.legend() # ATUALIZADO
plt.subplot(2, 3, 3); plt.plot(history.history['dice_coefficient'], label='Dice Coeff Treino'); plt.plot(history.history['val_dice_coefficient'], label='Dice Coeff Validação'); plt.title('Dice Coefficient'); plt.legend()
plt.subplot(2, 3, 4); plt.plot(history.history['precision'], label='Precision Treino'); plt.plot(history.history['val_precision'], label='Precision Validação'); plt.title('Precision'); plt.legend()
plt.subplot(2, 3, 5); plt.plot(history.history['recall'], label='Recall Treino'); plt.plot(history.history['val_recall'], label='Recall Validação'); plt.title('Recall'); plt.legend()
plt.subplot(2, 3, 6); plt.plot(history.history['f1_score'], label='F1-Score Treino'); plt.plot(history.history['val_f1_score'], label='F1-Score Validação'); plt.title('F1-Score'); plt.legend()
plt.tight_layout()
plt.savefig(GRAPH_SAVE_PATH)
print(f"Gráfico de performance salvo em: {GRAPH_SAVE_PATH}")
plt.show()

# Célula 4: Avaliação do Modelo Treinado (ATUALIZADA com Métricas Completas)
print("\n--- Iniciando Bloco 4: Avaliação do Modelo Treinado ---")

# --- Carregamento do Modelo ---
print(f"Carregando modelo de: {MODEL_CHECKPOINT_PATH}")
try:
    # Usamos o dicionário global CUSTOM_OBJECTS
    model = load_model(MODEL_CHECKPOINT_PATH, custom_objects=CUSTOM_OBJECTS)
    print("Modelo carregado com sucesso!")
except Exception as e:
    print(f"ERRO: Não foi possível carregar o modelo. Verifique se o caminho '{MODEL_CHECKPOINT_PATH}' está correto e se o treinamento foi concluído. Erro: {e}")
    # Pula o resto da célula se o modelo não puder ser carregado
    model = None

if model is not None:
    # --- Carregamento dos Dados de Validação ---
    # Reutilizamos os dados X_val_proc e y_val (definidos na Célula 3)
    # Se a Célula 3 não foi executada, precisamos recarregar os dados.
    # Esta é uma verificação de segurança:
    if 'X_val' not in locals() or 'y_val' not in locals():
        print("Recarregando dados de validação...")
        # (Este é um subconjunto da lógica de carregamento da Célula 3)
        image_files = sorted(os.listdir(IMAGES_PATH))
        mask_files = sorted(os.listdir(MASKS_PATH))
        X, y = [], []
        for img_file in image_files:
            mask_file = os.path.splitext(img_file)[0] + '.png'
            if mask_file in mask_files:
                img = cv2.imread(os.path.join(IMAGES_PATH, img_file))
                if img is None: continue
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                img = cv2.resize(img, (IMG_HEIGHT, IMG_WIDTH))
                X.append(img)
                mask = cv2.imread(os.path.join(MASKS_PATH, mask_file), cv2.IMREAD_GRAYSCALE)
                if mask is None:
                    X.pop()
                    continue
                mask = cv2.resize(mask, (IMG_HEIGHT, IMG_WIDTH))
                y.append(mask)
        X = np.array(X, dtype=np.float32)
        X = tf.keras.applications.mobilenet_v2.preprocess_input(X)
        y = np.array(y, dtype=np.float32) / 255.0
        y = np.expand_dims(y, axis=-1)
        _, X_val, _, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
        print("Dados de validação recarregados.")

    print(f"{len(X_val)} imagens de validação prontas para avaliação.")

    # --- Cálculo Final das Métricas ---
    scores = model.evaluate(X_val, y_val, verbose=1)

    print("\n--- Métricas Finais no Conjunto de Validação ---")

    # CORREÇÃO:
    # Ao carregar um modelo, Keras pode não preencher 'model.metrics_names' corretamente
    # para métricas baseadas em funções. Vamos acessar os 'scores' pela ordem
    # em que o modelo foi compilado, pois 'model.evaluate' retorna uma lista nessa ordem.
    # Ordem da compilação: loss, dice_coefficient, iou, precision, recall, f1_score

    if isinstance(scores, list) and len(scores) >= 6:
        print(f"Perda (Loss): {scores[0]:.4f}")
        print(f"Dice Coefficient: {scores[1]:.4f}")
        print(f"Mean IoU (Foreground): {scores[2]:.4f}") # ATUALIZADO
        print(f"Precision: {scores[3]:.4f}")
        print(f"Recall: {scores[4]:.4f}")
        print(f"F1-Score: {scores[5]:.4f}")
    else:
        # Fallback caso a ordem mude ou algo dê errado
        print("Não foi possível mapear os scores automaticamente. Usando 'model.metrics_names' como fallback:")
        metrics_dict = dict(zip(model.metrics_names, scores))
        # Usamos .get() sem formatação para evitar o erro, caso o valor seja 'N/A'
        print(f"Perda (Loss): {metrics_dict.get('loss', 'N/A')}")
        print(f"Dice Coefficient: {metrics_dict.get('dice_coefficient', 'N/A')}")
        print(f"Mean IoU (Foreground): {metrics_dict.get('iou_metric', 'N/A')}") # ATUALIZADO
        print(f"Precision: {metrics_dict.get('precision', 'N/A')}")
        print(f"Recall: {metrics_dict.get('recall', 'N/A')}")
        print(f"F1-Score: {metrics_dict.get('f1_score', 'N/A')}")

    # --- Visualização das Previsões ---
    # Precisamos recarregar as imagens originais (não processadas) para visualização
    print("Carregando imagens originais para visualização...")
    X_orig, y_orig = [], []
    for img_file in image_files:
        mask_file = os.path.splitext(img_file)[0] + '.png'
        if mask_file in mask_files:
            img = cv2.imread(os.path.join(IMAGES_PATH, img_file))
            if img is None: continue
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img = cv2.resize(img, (IMG_HEIGHT, IMG_WIDTH))
            X_orig.append(img)
            mask = cv2.imread(os.path.join(MASKS_PATH, mask_file), cv2.IMREAD_GRAYSCALE)
            if mask is None:
                X_orig.pop()
                continue
            mask = cv2.resize(mask, (IMG_HEIGHT, IMG_WIDTH))
            mask = np.expand_dims(mask, axis=-1)
            y_orig.append(mask)
    y_orig_np = np.array(y_orig, dtype=np.float32) / 255.0
    _, X_val_orig, _, y_val_orig = train_test_split(X_orig, y_orig_np, test_size=0.2, random_state=42)

    predictions = model.predict(X_val)
    predictions = (predictions > 0.5).astype(np.uint8)

    def display_results(images, true_masks, pred_masks, num_to_show=5):
        plt.figure(figsize=(15, num_to_show * 5))
        for i in range(num_to_show):
            plt.subplot(num_to_show, 3, i * 3 + 1); plt.title("Imagem Original"); plt.imshow(images[i]); plt.axis('off')
            plt.subplot(num_to_show, 3, i * 3 + 2); plt.title("Máscara Real"); plt.imshow(true_masks[i, :, :, 0], cmap='gray'); plt.axis('off')
            plt.subplot(num_to_show, 3, i * 3 + 3); plt.title("Máscara Prevista"); plt.imshow(pred_masks[i, :, :, 0], cmap='gray'); plt.axis('off')
        plt.tight_layout()
        plt.show()

    print("\n--- Visualização das Previsões ---")
    display_results(X_val_orig, y_val_orig, predictions, num_to_show=5)

# Célula 5: Análise de Novas Imagens (Inferência)
print("\n--- Iniciando Bloco 5: Aplicação do Modelo em Novas Imagens ---")

os.makedirs(RESULTS_MASKS_PATH, exist_ok=True)
os.makedirs(RESULTS_OVERLAYS_PATH, exist_ok=True)

# --- Carregar o Modelo de Segmentação ---
print(f"Carregando modelo treinado de: {MODEL_CHECKPOINT_PATH}")
try:
    # Para inferência, podemos usar compile=False, mas ainda precisamos dos custom_objects
    # se a função de perda não for padrão do Keras.
    model_inference = load_model(MODEL_CHECKPOINT_PATH, custom_objects=CUSTOM_OBJECTS, compile=False)
    print("Modelo de inferência carregado com sucesso.")
except Exception as e:
    print(f"ERRO CRÍTICO ao carregar o modelo: {e}")
    model_inference = None

if model_inference is not None:
    # --- Função de Pré-processamento para Inferência ---
    def preprocess_inference_image(image_path, target_height, target_width):
        img = cv2.imread(image_path)
        if img is None:
            raise IOError(f"Não foi possível ler a imagem: {image_path}")
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img_resized_original = cv2.resize(img_rgb, (target_width, target_height))
        img_float = img_resized_original.astype(np.float32)
        img_preprocessed = tf.keras.applications.mobilenet_v2.preprocess_input(img_float)
        return img_preprocessed, img_resized_original

    # --- Processamento em Lote das Novas Imagens ---
    image_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff')
    new_image_files = sorted([f for f in os.listdir(NEW_IMAGES_PATH) if f.lower().endswith(image_extensions) and os.path.isfile(os.path.join(NEW_IMAGES_PATH, f))])

    if not new_image_files:
        print(f"AVISO: Nenhuma imagem encontrada no diretório: {NEW_IMAGES_PATH}")
    else:
        print(f"\nIniciando predição para {len(new_image_files)} novas imagens...")
        for filename in new_image_files:
            image_path = os.path.join(NEW_IMAGES_PATH, filename)
            try:
                processed_img, resized_original_img = preprocess_inference_image(image_path, IMG_HEIGHT, IMG_WIDTH)
                predicted_mask_raw = model_inference.predict(np.expand_dims(processed_img, axis=0))[0]
                pred_max = np.max(predicted_mask_raw)
                print(f"   - Processando '{filename}': Valor Máximo da Predição = {pred_max:.4f}")
                binary_mask = (predicted_mask_raw > 0.5).astype(np.uint8)

                if np.sum(binary_mask) == 0:
                    print(f"     -> AVISO: Nenhuma mucosa foi segmentada em '{filename}'.")

                mask_save_path = os.path.join(RESULTS_MASKS_PATH, f"mascara_{filename}")
                cv2.imwrite(mask_save_path, binary_mask * 255)

                overlay_img = cv2.bitwise_and(resized_original_img, resized_original_img, mask=binary_mask)
                overlay_save_path = os.path.join(RESULTS_OVERLAYS_PATH, f"recorte_{filename}")
                cv2.imwrite(overlay_save_path, cv2.cvtColor(overlay_img, cv2.COLOR_RGB2BGR))

            except Exception as e:
                print(f"   - ERRO CRÍTICO ao processar '{filename}': {e}")

        print("\nAnálise de novas imagens concluída!")
        print(f"Verifique os resultados em:\n   - Máscaras: {RESULTS_MASKS_PATH}\n   - Recortes: {RESULTS_OVERLAYS_PATH}")

# Célula 6: Comparação Visual dos Resultados da Inferência
print("\n--- Iniciando Bloco 6: Comparação Visual dos Resultados da Inferência ---")

image_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff')
original_files = sorted([f for f in os.listdir(NEW_IMAGES_PATH) if f.lower().endswith(image_extensions) and os.path.isfile(os.path.join(NEW_IMAGES_PATH, f))])
predicted_files = sorted([f for f in os.listdir(RESULTS_OVERLAYS_PATH) if f.lower().endswith(image_extensions) and os.path.isfile(os.path.join(RESULTS_OVERLAYS_PATH, f))])

if not original_files:
    print(f"AVISO: Nenhuma imagem original encontrada em: {NEW_IMAGES_PATH}")
elif not predicted_files:
    print(f"AVISO: Nenhum recorte previsto encontrado em: {RESULTS_OVERLAYS_PATH}")
else:
    print(f"Encontradas {len(original_files)} imagens originais e {len(predicted_files)} recortes para comparar.")

    num_to_show = len(original_files)
    plt.figure(figsize=(10, num_to_show * 5))
    plot_index = 1

    for original_filename in original_files:
        predicted_filename = f"recorte_{original_filename}"
        if predicted_filename in predicted_files:
            original_path = os.path.join(NEW_IMAGES_PATH, original_filename)
            predicted_path = os.path.join(RESULTS_OVERLAYS_PATH, predicted_filename)

            img_original = cv2.imread(original_path)
            img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)
            img_predicted = cv2.imread(predicted_path)
            img_predicted = cv2.cvtColor(img_predicted, cv2.COLOR_RGB2BGR)

            plt.subplot(num_to_show, 2, plot_index); plt.title(f"Original: {original_filename}"); plt.imshow(img_original); plt.axis('off')
            plot_index += 1
            plt.subplot(num_to_show, 2, plot_index); plt.title(f"Recorte Previsto"); plt.imshow(img_predicted); plt.axis('off')
            plot_index += 1
        else:
            print(f"   - AVISO: Recorte para '{original_filename}' não encontrado.")

    plt.tight_layout()
    plt.show()

print("\n--- Fim do Script ---")

#ADAPTADOOOOOOOOOOOOOOOOOOOOOOOOOO PARA CLASSIFICAÇÃO DA MUCOSA
print("\n--- CÉLULA 5 (Adaptada): Aplicação do Modelo de Segmentação ---")

# Importações adicionais necessárias para este snippet
import cv2
from tensorflow.keras.models import load_model

# --- DEFINA OS CAMINHOS AQUI ---
# BASE_PATH já deve existir da Célula 3
BASE_PATH = "/content/drive/MyDrive/projeto_tc_ovinos/classificacaoMucosa/"

# 1. Caminho para o SEU MODELO DE SEGMENTAÇÃO TREINADO (ex: U-Net)
# !!! ATENÇÃO: ESTE NÃO É O MODELO DE CLASSIFICAÇÃO 'anemia_model_final.keras' !!!
# !!!          Este deve ser o modelo que você treinou para GERAR MÁSCARAS. !!!
MODEL_CHECKPOINT_PATH = os.path.join(DRIVE_BASE_PATH, "melhor_modelo_unet_metricas_completas.keras") # <--- MUDE AQUI

# 2. Caminhos de Saída
RESULTS_MASKS_PATH = os.path.join(BASE_PATH, "Resultados_Mascaras")
RESULTS_RECORTES_PATH = os.path.join(BASE_PATH, "Resultados_Recortes") # Nome da pasta "recorte"

os.makedirs(RESULTS_MASKS_PATH, exist_ok=True)
os.makedirs(RESULTS_RECORTES_PATH, exist_ok=True)

# --- Definição de Objetos Customizados (se houver) ---
# Se seu modelo de segmentação usou Dice Loss ou IoU,
# você precisa defini-los aqui para o load_model funcionar.
# Exemplo:
# def dice_loss(y_true, y_pred):
#     ...
# CUSTOM_OBJECTS = {'dice_loss': dice_loss}
CUSTOM_OBJECTS = {} # Deixe vazio se não usou nada customizado

# --- Carregar o Modelo de Segmentação ---
print(f"Carregando modelo de SEGMENTAÇÃO de: {MODEL_CHECKPOINT_PATH}")
try:
    model_inference = load_model(MODEL_CHECKPOINT_PATH, custom_objects=CUSTOM_OBJECTS, compile=False)
    print("Modelo de inferência (segmentação) carregado com sucesso.")
except Exception as e:
    print(f"ERRO CRÍTICO ao carregar o modelo: {e}")
    print(f"Verifique se o MODEL_CHECKPOINT_PATH ('{MODEL_CHECKPOINT_PATH}') está correto.")
    model_inference = None

if model_inference is not None:
    # --- Função de Pré-processamento para Inferência ---
    # (Vou assumir que o modelo de segmentação foi treinado com os mesmos inputs)
    def preprocess_inference_image(image_path, target_height, target_width):
        img = cv2.imread(image_path)
        if img is None:
            raise IOError(f"Não foi possível ler a imagem: {image_path}")
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img_resized_original = cv2.resize(img_rgb, (target_width, target_height))

        # Use o pré-processamento correto para o seu modelo de segmentação
        # O snippet original usava MobileNetV2, vou manter isso.
        # Se seu modelo não usa isso (ex: só /255), ajuste aqui.
        img_float = img_resized_original.astype(np.float32)
        img_preprocessed = tf.keras.applications.mobilenet_v2.preprocess_input(img_float)
        return img_preprocessed, img_resized_original

    # --- (MUDANÇA PRINCIPAL) Descoberta de Imagens nas Subpastas Numéricas ---
    print(f"Escaneando subpastas numéricas em: {BASE_PATH}")
    all_image_paths = []
    image_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff')

    for folder_name in sorted(os.listdir(BASE_PATH)):
        folder_path = os.path.join(BASE_PATH, folder_name)

        # Procura pastas com nome puramente numérico (ex: "1", "25")
        if os.path.isdir(folder_path) and folder_name.isdigit():
            print(f"  Analisando pasta: {folder_name}...")
            try:
                for file_name in sorted(os.listdir(folder_path)):
                    if file_name.lower().endswith(image_extensions):
                        filepath = os.path.join(folder_path, file_name)
                        # Salva (ID da ovelha, nome do arquivo, caminho completo)
                        all_image_paths.append((folder_name, file_name, filepath))
            except Exception as e:
                print(f"    Erro ao ler a pasta {folder_name}: {e}")

    if not all_image_paths:
        print(f"AVISO: Nenhuma imagem encontrada nas subpastas numéricas de: {BASE_PATH}")
    else:
        print(f"\nIniciando predição para {len(all_image_paths)} imagens de {len(np.unique([p[0] for p in all_image_paths]))} ovelhas...")

        # --- (MUDANÇA PRINCIPAL) Processamento em Lote das Novas Imagens ---
        for id_ovelha, filename, image_path in all_image_paths:
            try:
                # (IMG_HEIGHT e IMG_WIDTH vêm da Célula 2)
                processed_img, resized_original_img = preprocess_inference_image(image_path, IMG_HEIGHT, IMG_WIDTH)

                # Previsão
                predicted_mask_raw = model_inference.predict(np.expand_dims(processed_img, axis=0))[0]

                # --- Ajuste de Pós-processamento (SE NECESSÁRIO) ---
                # Se a saída do seu modelo for (H, W, 1), o limiar 0.5 funciona.
                # Se a saída for (H, W, N_CLASSES), (ex: [fundo, mucosa])
                # você precisa pegar o canal correto:
                # Ex: predicted_mask_raw = predicted_mask_raw[:, :, 1]
                # --- Fim do Ajuste ---

                binary_mask = (predicted_mask_raw > 0.5).astype(np.uint8)

                if np.sum(binary_mask) == 0:
                    print(f"   -> AVISO: Nenhuma mucosa foi segmentada em '{id_ovelha}/{filename}'.")

                # (MUDANÇA PRINCIPAL) Criar nomes de arquivo únicos (ex: 1_imagem.jpg)
                output_filename = f"{id_ovelha}_{filename}"

                # Salvar a máscara
                mask_save_path = os.path.join(RESULTS_MASKS_PATH, f"mascara_{output_filename}")
                cv2.imwrite(mask_save_path, binary_mask * 255) # Salva como P&B

                # Aplicar máscara na imagem original para criar o recorte
                if binary_mask.ndim == 3 and binary_mask.shape[-1] == 1:
                    binary_mask = binary_mask.squeeze(-1) # Converte (H, W, 1) para (H, W)

                overlay_img = cv2.bitwise_and(resized_original_img, resized_original_img, mask=binary_mask)

                # Salvar o recorte
                overlay_save_path = os.path.join(RESULTS_RECORTES_PATH, f"recorte_{output_filename}")
                cv2.imwrite(overlay_save_path, cv2.cvtColor(overlay_img, cv2.COLOR_RGB2BGR)) # Salva como BGR

            except Exception as e:
                print(f"   - ERRO CRÍTICO ao processar '{id_ovelha}/{filename}': {e}")

        print("\nAnálise de novas imagens concluída!")
        print(f"Verifique os resultados em:\n   - Máscaras: {RESULTS_MASKS_PATH}\n   - Recortes: {RESULTS_RECORTES_PATH}")